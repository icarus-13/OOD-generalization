{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21951,"status":"ok","timestamp":1692710722339,"user":{"displayName":"Yahuan Zheng","userId":"00052934426780333783"},"user_tz":-120},"id":"eCfejD502gy5","outputId":"f87f80b8-cd8d-416c-ebd9-8c7d81e7cf2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Datasets\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1SCi4Ejw83a"},"outputs":[],"source":["import torch\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","import matplotlib.pyplot as plt\n","\n","def set_device():\n","    if torch.cuda.is_available():\n","        dev = \"cuda:0\"\n","    else:\n","        dev = \"cpu\"\n","    return torch.device(dev)\n","\n","class Logger:\n","    def __init__(self):\n","        self.data = dict()\n","\n","    def log(self, k, v):\n","        if not k in self.data:\n","            self.data[k] = []\n","        v = self._to_np(v)\n","        self.data[k].append(v)\n","\n","    def __getitem__(self, k):\n","        return self.data.get(k, [])\n","\n","    def _to_np(self, v):\n","        if isinstance(v, torch.Tensor):\n","            with torch.no_grad():\n","                return v.cpu().numpy()\n","        if isinstance(v, list):\n","            return [self._to_np(v_) for v_ in v]\n","        return v\n","\n","def plot(log):\n","    plt.figure()\n","    plt.semilogy(log['irm_penalty'])\n","    plt.semilogy(log['rex_penalty'])\n","    plt.legend(['irm penalty', 'rex penalty'])\n","    plt.figure()\n","    plt.plot(log['train_acc'], 'k')\n","    plt.plot(log['test_acc'], alpha=0.7)\n","    plt.legend(['train acc'] + ['test acc'] * len(log['test_acc'][0]))\n","    plt.figure()\n","    plt.plot(log['losses'], 'k', alpha=0.3)\n","    plt.title('losses')\n","    plt.ylabel('nll')\n","    plt.show()\n","\n","def save(logs, name):\n","    data = dict()\n","    for k in logs[0].data.keys():\n","        data[k] = np.array([l[k] for l in logs])\n","        np.savetxt('%s_%s_mean.dat' % (name, k), data[k].mean(0))\n","        np.savetxt('%s_%s_std.dat' % (name, k), data[k].std(0))\n","    np.savez_compressed('%s.npz' % name, **data)\n","\n","def print_stats(step, log):\n","    pretty_print(\n","        np.int32(step),\n","        np.mean(log['train_nll'][-50:]),\n","        np.mean(log['train_acc'][-50:]),\n","        np.mean(log['irm_penalty'][-50:]),\n","        np.mean(log['rex_penalty'][-50:]),\n","        np.mean(log['test_acc'][-50:])\n","        #*np.array(log['test_acc'][-50:]).mean(axis=0),\n","    )\n","\n","def pretty_print(*values):\n","    col_width = 13\n","    def format_val(v):\n","        if not isinstance(v, str):\n","            v = np.array2string(v, precision=5, floatmode='fixed')\n","        return v.ljust(col_width)\n","    str_values = [format_val(v) for v in values]\n","    print(\"   \".join(str_values))\n","\n","def print_env_info(train_envs, test_envs):\n","    num_feat = train_envs[0]['features'].shape[1]\n","    print('training on', len(train_envs), 'environments (using', num_feat, 'features):')\n","    for e in train_envs:\n","        print('   ', e['info'], 'with', len(e['labels']), 'samples')\n","    print('\\ntesting on:')\n","    for e in test_envs:\n","        print('   ', e['info'], 'with', len(e['labels']), 'samples')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84_NZjYbwHwn"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn import preprocessing\n","import torch\n","\n","YEARS = [2014, 2015, 2016, 2017, 2018]\n","\n","class ToNum:\n","    def __init__(self):\n","        self.symbols = []\n","\n","    def convert(self, symbol):\n","        if not symbol in self.symbols:\n","            self.symbols.append(symbol)\n","            return len(self.symbols) - 1\n","        else:\n","            return self.symbols.index(symbol)\n","\n","    def index(self, key):\n","        return self.symbols.index(key)\n","\n","\n","def get_years(years=[], anchors=[]):\n","    device = set_device()\n","\n","    # Get the full data across training and testing environments\n","    data_all = [pd.read_csv('data/%s_clean.csv' % y, index_col=0) for y in YEARS]\n","\n","    k = data_all[0].keys()\n","    for d in data_all:\n","        k = k.intersection(d.keys())\n","    #print(k)\n","\n","    data_all = [d[k] for d in data_all]\n","    # stack everything\n","    _data = None\n","    for d in data_all:\n","        if _data is None:\n","            _data = d\n","        else:\n","            _data = pd.concat([_data, d])\n","    data_all = _data\n","\n","    # Yahuan Zheng: Build the sector mapping\n","    sectors = data_all['Sector'].unique()\n","    le = preprocessing.LabelEncoder()\n","    sector_labels = le.fit_transform(sectors)\n","    sector_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n","\n","    # Yahuan Zheng: Get the training data and add an additional column for sector labelling\n","    data = [pd.read_csv('data/%s_clean.csv' % y, index_col=0) for y in years]\n","    data = [pd.concat([d[k], d.iloc[:,-1:]], axis = 1) for d in data]  # Yahuan Zheng: IMPORTANT!!! Add the final column consisting of annual returns (otherwise discarded)\n","\n","    for d in data:\n","        d['Sector_label'] = d['Sector'].apply(lambda x: sector_mapping[x])\n","\n","    k_data = k.drop(['Sector', 'Class'])   # Sector_label is not included in k in the first place\n","    k_target = 'Class'\n","\n","    # Yahuan Zheng: Shuffle the data\n","    data_shuffled = []\n","    for d in data:\n","        num_0 = d['Class'].isin([0]).sum()\n","        num_1 = d['Class'].isin([1]).sum()\n","        n = min(num_0, num_1)\n","        class_0 = d.nsmallest(n, 'Class')\n","        class_1 = d.nlargest(n, 'Class')\n","        d_shuffled = pd.concat([class_0, class_1]).sample(frac=1)\n","        #d_shuffled = class_0.append(class_1).sample(frac=1)\n","        data_shuffled.append(d_shuffled)    # returns a list of dataframes for the desired years\n","    data = data_shuffled\n","\n","    x = [d[k_data].to_numpy() for d in data]\n","    y = [d[k_target].to_numpy() for d in data]\n","    x_anchors = [d[anchors].to_numpy() for d in data]   # Yahuan Zheng: add the anchor variables\n","    r = [d.iloc[:,-2:-1].to_numpy() for d in data]        # Yahuan Zheng: add the annual returns\n","\n","\n","    return [{\n","        'features': torch.tensor(x_, dtype=torch.float32).to(device),\n","        'labels': torch.tensor(y_.reshape(-1, 1), dtype=torch.float32).to(device),\n","        'anchors': torch.tensor(a_, dtype=torch.float32).to(device),\n","        'returns': torch.tensor(r_, dtype=torch.float32).to(device),\n","        'info': yr_,\n","        } for x_, y_, a_, r_, yr_ in zip(x, y, x_anchors, r, years)]\n","\n","\n","def get_sectors():\n","    device = set_device()\n","    data_all = [pd.read_csv('data/%s_clean.csv' % y, index_col=0) for y in YEARS]\n","\n","    k = data_all[0].keys()\n","    for d in data_all:\n","        k = k.intersection(d.keys())\n","\n","    data = [d[k] for d in data_all]\n","\n","    # stack everything\n","    _data = None\n","    for d in data:\n","        if _data is None:\n","            _data = d\n","        else:\n","            _data = pd.concat([_data, d])\n","    data = _data\n","\n","    # sort by sectors and shuffle the data within each sector\n","    sectors = data['Sector'].unique()\n","    _data = []\n","    for s in sectors:\n","        ids = data['Sector'].isin([s])\n","        _data = pd.concat([_data, data[ids].sample(frac=1)])\n","    data = _data\n","\n","    data_shuffled = []\n","    for d in data:\n","        num_0 = d['Class'].isin([0]).sum()\n","        num_1 = d['Class'].isin([1]).sum()\n","        n = min(num_0, num_1)\n","        class_0 = d.nsmallest(n, 'Class')\n","        class_1 = d.nlargest(n, 'Class')\n","        d_shuffled = pd.concat([class_0, class_1 ]).sample(frac=1)\n","        data_shuffled.append(d_shuffled)\n","    data = data_shuffled\n","\n","    k_data = k.drop(['Sector', 'Class'])\n","    k_target = 'Class'\n","\n","    x = [d[k_data].to_numpy() for d in data]\n","    y = [d[k_target].to_numpy() for d in data]\n","\n","    return [{\n","        'features': torch.tensor(x_, dtype=torch.float32).to(device),\n","        'labels': torch.tensor(y_.reshape(-1, 1), dtype=torch.float32).to(device),\n","        'info': s_\n","        } for x_, y_, s_ in zip(x, y, sectors)]\n","\n","# if __name__ == '__main__':\n","\n","#    x, y = get_envs()\n"]},{"cell_type":"markdown","source":["# For ERM, IRM and REx:"],"metadata":{"id":"BKmE5v_2tAwK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJUz_xf2vhLV"},"outputs":[],"source":["# Use CLASS LABEL as the target variable\n","# Run ERM, IRM and REx\n","\n","import argparse\n","import numpy as np\n","import numpy.lib as npl\n","import torch\n","import matplotlib.pyplot as plt\n","from torch import nn, optim, autograd\n","from sklearn.linear_model import LinearRegression\n","#from load import get_years, get_sectors, YEARS\n","#from utils import *\n","\n","parser = argparse.ArgumentParser(description='Finance Experiment')\n","parser.add_argument('--hidden_dim', type=int, default=256)\n","parser.add_argument('--l2_regularizer_weight', type=float,default=0.001)\n","parser.add_argument('--lr', type=float, default=0.001)\n","parser.add_argument('--n_restarts', type=int, default=10)\n","parser.add_argument('--penalty_anneal_iters', type=int, default=100)\n","parser.add_argument('--irm_penalty_weight', type=float, default=0.0)\n","parser.add_argument('--rex_penalty_weight', type=float, default=10000.0)      # Can adjust the IRM and REx penalty weight to choose which method to perform. Setting both to 0 equals ERM.\n","parser.add_argument('--steps', type=int, default=501)\n","parser.add_argument('--plot', action='store_true')\n","parser.add_argument('--save', type=str, default='')\n","parser.add_argument('--train_envs', type=str, default='2014,2015,2016')\n","parser.add_argument('--test_envs', type=str, default='')\n","flags, unknown = parser.parse_known_args()\n","\n","train_env_ids = [int(s.strip()) for s in flags.train_envs.split(',')]\n","if flags.test_envs:\n","    test_env_ids = [int(s.strip()) for s in flags.test_envs.split(',')]\n","else:\n","    test_env_ids = npl.setxor1d(YEARS, train_env_ids)\n","\n","\n","print('Flags:')\n","for k,v in sorted(vars(flags).items()):\n","    print(\"    {}: {}\".format(k, v))\n","\n","\n","def whiten(x):\n","    with torch.no_grad():\n","        x -= x.mean(dim=0)\n","        x /= x.std(dim=0)\n","    return x\n","\n","def mean_nll(logits, y):\n","    return nn.functional.binary_cross_entropy_with_logits(logits, y)\n","\n","def mean_accuracy(logits, y):\n","    preds = (logits > 0.).float()\n","    return ((preds - y).abs() < 1e-2).float().mean()\n","\n","def env_irm_penalty(logits, y):\n","    device = set_device()\n","    scale = torch.tensor(1.).to(device).requires_grad_()\n","    loss = mean_nll(logits * scale, y)\n","    grad = autograd.grad(loss, [scale], create_graph=True)[0]\n","    return torch.mean(grad**2)\n","\n","def get_rex_penalty(train_envs):\n","    losses = torch.stack([e['nll'] for e in train_envs])\n","    penalty = torch.var(losses)\n","    return penalty\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_size):\n","        super(MLP, self).__init__()\n","        self.input_size = input_size\n","        lin1 = nn.Linear(input_size, flags.hidden_dim)\n","        lin2 = nn.Linear(flags.hidden_dim, flags.hidden_dim)\n","        lin3 = nn.Linear(flags.hidden_dim, 1)\n","        for lin in [lin1, lin2, lin3]:\n","            nn.init.xavier_uniform_(lin.weight)\n","            nn.init.zeros_(lin.bias)\n","        self._main = nn.Sequential(\n","            lin1, nn.Tanh(), #nn.ReLU(True),\n","            nn.Dropout(),\n","            lin2, nn.Tanh(), #nn.ReLU(True),\n","            nn.Dropout(),\n","            lin3)\n","\n","    def forward(self, x):\n","        x = x.view(x.shape[0], self.input_size)\n","        out = self._main(x)\n","        return out\n","\n","\n","final_train_accs = []\n","final_test_accs = []\n","logs = []\n","for restart in range(flags.n_restarts):\n","    print(\"\\n-------------------------------------- Restart\", restart+1, \"--------------------------------------\\n\")\n","\n","    train_envs = get_years(train_env_ids)\n","    test_envs = get_years(test_env_ids)\n","    # preprocess\n","    for e in train_envs + test_envs:\n","        e['features'] = whiten(e['features'])\n","        e['anchors'] = whiten(e['anchors'])\n","    print_env_info(train_envs, test_envs)\n","\n","    # init\n","    device = set_device()\n","    logger = Logger()\n","    mlp = MLP(train_envs[0]['features'].shape[1]).to(device)\n","    optimizer = optim.Adam(mlp.parameters(), lr=flags.lr)\n","\n","    pretty_print('\\n\\nstep', 'train nll', 'train acc', 'irm penalty', 'rex penalty', 'test acc')\n","\n","    for step in range(flags.steps):\n","        for env in train_envs + test_envs:\n","            env['logits'] = mlp(env['features'])\n","            env['nll'] = mean_nll(env['logits'], env['labels'])\n","            env['acc'] = mean_accuracy(env['logits'], env['labels'])\n","            env['penalty'] = env_irm_penalty(env['logits'], env['labels'])\n","\n","        train_nll = torch.stack([e['nll'] for e in train_envs]).mean()\n","        train_acc = torch.stack([e['acc'] for e in train_envs]).mean()\n","        irm_penalty = torch.stack([e['penalty'] for e in train_envs]).mean()\n","        rex_penalty = get_rex_penalty(train_envs)\n","\n","        weight_norm = torch.tensor(0.).to(device)\n","        for w in mlp.parameters():\n","            weight_norm += w.norm().pow(2)\n","\n","        loss = train_nll.clone()\n","        loss += flags.l2_regularizer_weight * weight_norm\n","\n","\n","        if flags.irm_penalty_weight:\n","            if step >= flags.penalty_anneal_iters:\n","                loss /= flags.irm_penalty_weight\n","            loss += irm_penalty\n","\n","        elif flags.rex_penalty_weight:\n","            if step >= flags.penalty_anneal_iters:\n","                loss /= flags.rex_penalty_weight\n","            loss += rex_penalty\n","\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        logger.log('train_nll', train_nll)\n","        logger.log('train_acc', train_acc)\n","        logger.log('irm_penalty', irm_penalty)\n","        logger.log('rex_penalty', rex_penalty)\n","        logger.log('test_acc', [e['acc'] for e in test_envs])\n","        logger.log('losses', [e['nll'] for e in train_envs])\n","\n","        if step % 100 == 0:\n","            print_stats(step, logger)\n","\n","    final_train_accs.append(np.mean(logger['train_acc'][-50:]))\n","    final_test_accs.append(np.mean(logger['test_acc'][-50:]))\n","    print('\\n\\nFinal train acc (mean/std across restarts so far):')\n","    print(np.mean(final_train_accs), np.std(final_train_accs))\n","    print('\\nFinal test acc (mean/std across restarts so far):')\n","    print(np.mean(final_test_accs), np.std(final_test_accs))\n","\n","    logs.append(logger)\n","\n","    if flags.plot:\n","        plot(logger)\n","\n","if flags.save:\n","    save(logs, 'results/%s_%s_%s' % ('', ','.join([str(e) for e in train_env_ids]), ','.join([str(e) for e in test_env_ids])))\n","\n"]},{"cell_type":"code","source":["# Use ANNUAL RETURN as the target variable\n","# Run ERM, IRM and REx\n","\n","import argparse\n","import numpy as np\n","import numpy.lib as npl\n","import torch\n","import matplotlib.pyplot as plt\n","from torch import nn, optim, autograd\n","from sklearn.linear_model import LinearRegression\n","#from load import get_years, get_sectors, YEARS\n","#from utils import *\n","\n","\n","parser = argparse.ArgumentParser(description='Finance Experiment')\n","parser.add_argument('--hidden_dim', type=int, default=256)\n","parser.add_argument('--l2_regularizer_weight', type=float,default=0.001)\n","parser.add_argument('--lr', type=float, default=0.001)\n","parser.add_argument('--n_restarts', type=int, default=10)\n","parser.add_argument('--penalty_anneal_iters', type=int, default=100)\n","parser.add_argument('--irm_penalty_weight', type=float, default=0.0)\n","parser.add_argument('--rex_penalty_weight', type=float, default=10000.0)      # Can adjust the IRM and REx penalty weight to choose which method to perform. Setting both to 0 equals ERM\n","parser.add_argument('--steps', type=int, default=501)\n","parser.add_argument('--plot', action='store_true')\n","parser.add_argument('--save', type=str, default='')\n","parser.add_argument('--train_envs', type=str, default='2014,2015,2016')\n","parser.add_argument('--test_envs', type=str, default='')\n","flags, unknown = parser.parse_known_args()\n","\n","train_env_ids = [int(s.strip()) for s in flags.train_envs.split(',')]\n","if flags.test_envs:\n","    test_env_ids = [int(s.strip()) for s in flags.test_envs.split(',')]\n","else:\n","    test_env_ids = npl.setxor1d(YEARS, train_env_ids)\n","\n","\n","print('Flags:')\n","for k,v in sorted(vars(flags).items()):\n","    print(\"    {}: {}\".format(k, v))\n","\n","\n","def whiten(x):\n","    with torch.no_grad():\n","        x -= x.mean(dim=0)\n","        x /= x.std(dim=0)\n","    return x\n","\n","\n","def env_irm_penalty(y_hat, y):\n","    device = set_device()\n","    scale = torch.tensor(1.).to(device).requires_grad_()\n","    loss = torch.nn.functional.mse_loss(y_hat * scale, y)\n","    grad = autograd.grad(loss, [scale], create_graph=True)[0]\n","    return torch.mean(grad**2)\n","\n","def get_rex_penalty(train_envs):\n","    losses = torch.stack([e['MSE'] for e in train_envs])      # Yahuan Zheng: Changed to MSE to use annual returns as target variable\n","    penalty = torch.var(losses)\n","    return penalty\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_size):\n","        super(MLP, self).__init__()\n","        self.input_size = input_size\n","        lin1 = nn.Linear(input_size, flags.hidden_dim)\n","        lin2 = nn.Linear(flags.hidden_dim, flags.hidden_dim)\n","        lin3 = nn.Linear(flags.hidden_dim, 1)\n","        for lin in [lin1, lin2, lin3]:\n","            nn.init.xavier_uniform_(lin.weight)\n","            nn.init.zeros_(lin.bias)\n","        self._main = nn.Sequential(\n","            lin1, nn.Tanh(), #nn.ReLU(True),\n","            nn.Dropout(),\n","            lin2, nn.Tanh(), #nn.ReLU(True),\n","            nn.Dropout(),\n","            lin3)\n","\n","    def forward(self, x):\n","        x = x.view(x.shape[0], self.input_size)\n","        out = self._main(x)\n","        return out\n","\n","final_train_mse = []\n","final_test_mse = []\n","final_train_acc = []\n","final_test_acc = []\n","logs = []\n","\n","for restart in range(flags.n_restarts):\n","    print(\"\\n-------------------------------------- Restart\", restart+1, \"--------------------------------------\\n\")\n","\n","    train_envs = get_years(train_env_ids)\n","    test_envs = get_years(test_env_ids)\n","    # preprocess\n","    for e in train_envs + test_envs:\n","        e['features'] = whiten(e['features'])\n","        e['anchors'] = whiten(e['anchors'])\n","        e['returns'] = whiten(e['returns'])\n","    print_env_info(train_envs, test_envs)\n","\n","    # init\n","    device = set_device()\n","    logger = Logger()\n","    mlp = MLP(train_envs[0]['features'].shape[1]).to(device)\n","    optimizer = optim.Adam(mlp.parameters(), lr=flags.lr)\n","\n","    pretty_print('\\n\\nstep', 'train mse', 'train acc', 'irm penalty', 'rex penalty', 'test mse', 'test acc')\n","\n","    for step in range(flags.steps):\n","        for env in train_envs + test_envs:\n","            env['preds'] = mlp(env['features'])     # Predict the annual return\n","            env['preds_labels'] = (env['preds'] > 0.).float()   # Assign the label based on the predicted return\n","            env['MSE'] = torch.nn.functional.mse_loss(env['preds'], env['returns'])\n","            env['acc'] = ((env['preds_labels'] - env['labels']).abs() < 1e-2).float().mean()\n","            env['penalty'] = env_irm_penalty(env['preds'], env['returns'])\n","\n","        train_mse = torch.stack([e['MSE'] for e in train_envs]).mean()\n","        train_acc = torch.stack([e['acc'] for e in train_envs]).mean()\n","        irm_penalty = torch.stack([e['penalty'] for e in train_envs]).mean()\n","        rex_penalty = get_rex_penalty(train_envs)\n","\n","        weight_norm = torch.tensor(0.).to(device)\n","        for w in mlp.parameters():\n","            weight_norm += w.norm().pow(2)\n","\n","        loss = train_mse.clone()  # Yahuan Zheng: changed to MSE to use annual return as target variable\n","        loss += flags.l2_regularizer_weight * weight_norm\n","\n","\n","        if flags.irm_penalty_weight:\n","            if step >= flags.penalty_anneal_iters:\n","                loss /= flags.irm_penalty_weight\n","            loss += irm_penalty\n","\n","        elif flags.rex_penalty_weight:\n","            if step >= flags.penalty_anneal_iters:\n","                loss /= flags.rex_penalty_weight\n","            loss += rex_penalty\n","\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        logger.log('train_mse', train_mse)\n","        logger.log('train_acc', train_acc)\n","        logger.log('irm_penalty', irm_penalty)\n","        logger.log('rex_penalty', rex_penalty)\n","        logger.log('test_mse', [e['MSE'] for e in test_envs])\n","        logger.log('test_acc', [e['acc'] for e in test_envs])\n","\n","        if step % 100 == 0:\n","            pretty_print(\n","                          np.int32(step),\n","                          np.mean(logger['train_mse'][-50:]),\n","                          np.mean(logger['train_acc'][-50:]),\n","                          np.mean(logger['irm_penalty'][-50:]),\n","                          np.mean(logger['rex_penalty'][-50:]),\n","                          np.mean(logger['test_mse'][-50:]),\n","                          np.mean(logger['test_acc'][-50:])\n","                      )\n","\n","    final_train_mse.append(np.mean(logger['train_mse'][-50:]))\n","    final_test_mse.append(np.mean(logger['test_mse'][-50:]))\n","    final_train_acc.append(np.mean(logger['train_acc'][-50:]))\n","    final_test_acc.append(np.mean(logger['test_acc'][-50:]))\n","\n","    print('\\n\\nFinal train mse (mean/std across restarts so far):')\n","    print(np.mean(final_train_mse), np.std(final_train_mse))\n","    print('\\nFinal train acc (mean/std across restarts so far):')\n","    print(np.mean(final_train_acc), np.std(final_train_acc))\n","    print('\\nFinal test mse (mean/std across restarts so far):')\n","    print(np.mean(final_test_mse), np.std(final_test_mse))\n","    print('\\nFinal test acc (mean/std across restarts so far):')\n","    print(np.mean(final_test_acc), np.std(final_test_acc))\n","\n","    logs.append(logger)\n","\n","    if flags.plot:\n","        plot(logger)\n","\n","if flags.save:\n","    save(logs, 'results/%s_%s_%s' % ('', ','.join([str(e) for e in train_env_ids]), ','.join([str(e) for e in test_env_ids])))\n","\n"],"metadata":{"id":"kroksI_QH1Rr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692710867038,"user_tz":-120,"elapsed":70960,"user":{"displayName":"Yahuan Zheng","userId":"00052934426780333783"}},"outputId":"d5bcb98e-1711-4db0-930c-1a62b9b6bc27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Flags:\n","    hidden_dim: 256\n","    irm_penalty_weight: 0.0\n","    l2_regularizer_weight: 0.001\n","    lr: 0.001\n","    n_restarts: 10\n","    penalty_anneal_iters: 100\n","    plot: False\n","    rex_penalty_weight: 10000.0\n","    save: \n","    steps: 501\n","    test_envs: \n","    train_envs: 2014,2015,2016\n","\n","-------------------------------------- Restart 1 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.59305         0.49158         1.33458         0.01011         1.57808         0.50429      \n","100             1.02274         0.54400         0.01130         0.00092         1.03560         0.52933      \n","200             1.01060         0.53222         0.00333         5.85353e-05     1.02505         0.52637      \n","300             1.01029         0.53303         0.00331         4.58486e-05     1.02560         0.52575      \n","400             1.00965         0.53361         0.00312         3.18283e-05     1.02426         0.52602      \n","500             1.00980         0.53210         0.00307         3.75802e-05     1.02409         0.52494      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.009805 0.0\n","\n","Final train acc (mean/std across restarts so far):\n","0.53210336 0.0\n","\n","Final test mse (mean/std across restarts so far):\n","1.024088 0.0\n","\n","Final test acc (mean/std across restarts so far):\n","0.52494323 0.0\n","\n","-------------------------------------- Restart 2 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.62685         0.51261         1.67254         0.00862         1.82606         0.44590      \n","100             1.04291         0.53954         0.02153         0.00090         1.05720         0.52519      \n","200             1.02189         0.52993         0.00665         5.61108e-05     1.03613         0.52418      \n","300             1.02405         0.52812         0.00693         5.83687e-05     1.03613         0.52351      \n","400             1.02186         0.52963         0.00644         5.63476e-05     1.03640         0.52511      \n","500             1.02252         0.52789         0.00655         7.76659e-05     1.03666         0.52442      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0161643 0.0063593984\n","\n","Final train acc (mean/std across restarts so far):\n","0.52999735 0.002105981\n","\n","Final test mse (mean/std across restarts so far):\n","1.0303745 0.006286502\n","\n","Final test acc (mean/std across restarts so far):\n","0.5246837 0.00025951862\n","\n","-------------------------------------- Restart 3 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.60135         0.50354         1.39279         0.00036         1.61515         0.51171      \n","100             1.04124         0.53947         0.02081         0.00097         1.05663         0.52585      \n","200             1.02227         0.53117         0.00634         5.57350e-05     1.03479         0.52762      \n","300             1.02209         0.53039         0.00625         5.20513e-05     1.03452         0.52946      \n","400             1.02141         0.53199         0.00605         6.34287e-05     1.03390         0.52783      \n","500             1.02137         0.53230         0.00606         5.32256e-05     1.03221         0.52801      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0178982 0.005742345\n","\n","Final train acc (mean/std across restarts so far):\n","0.53076345 0.0020323766\n","\n","Final test mse (mean/std across restarts so far):\n","1.0309874 0.005205585\n","\n","Final test acc (mean/std across restarts so far):\n","0.5257909 0.0015800842\n","\n","-------------------------------------- Restart 4 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.51807         0.49649         1.02023         4.71925e-05     1.47472         0.51341      \n","100             1.03303         0.53903         0.01566         0.00093         1.04450         0.52769      \n","200             1.01623         0.53454         0.00519         5.27685e-05     1.03022         0.52870      \n","300             1.01650         0.53434         0.00508         3.89640e-05     1.03104         0.52720      \n","400             1.01659         0.53412         0.00509         6.01837e-05     1.03133         0.52776      \n","500             1.01512         0.53476         0.00477         6.17247e-05     1.02868         0.52946      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0172036 0.0051164865\n","\n","Final train acc (mean/std across restarts so far):\n","0.5317626 0.0024683063\n","\n","Final test mse (mean/std across restarts so far):\n","1.0304108 0.004617512\n","\n","Final test acc (mean/std across restarts so far):\n","0.5267083 0.002096964\n","\n","-------------------------------------- Restart 5 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.50909         0.50336         1.04219         0.00010         1.58958         0.46946      \n","100             1.03655         0.53974         0.01807         0.00088         1.05132         0.52604      \n","200             1.01925         0.53142         0.00598         6.15360e-05     1.03298         0.52854      \n","300             1.02236         0.52935         0.00645         7.69597e-05     1.03421         0.52842      \n","400             1.01884         0.53102         0.00583         5.04011e-05     1.03339         0.52711      \n","500             1.02047         0.52910         0.00592         5.47391e-05     1.03171         0.52884      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0178571 0.0047592833\n","\n","Final train acc (mean/std across restarts so far):\n","0.53122985 0.0024514035\n","\n","Final test mse (mean/std across restarts so far):\n","1.0306697 0.004162363\n","\n","Final test acc (mean/std across restarts so far):\n","0.52713364 0.0020594925\n","\n","-------------------------------------- Restart 6 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.57587         0.48330         1.21537         0.00359         1.54446         0.51675      \n","100             1.02926         0.54073         0.01407         0.00084         1.04280         0.52505      \n","200             1.01444         0.53310         0.00415         5.43912e-05     1.02052         0.53400      \n","300             1.01290         0.53323         0.00393         4.98904e-05     1.01944         0.53445      \n","400             1.01235         0.53356         0.00381         4.42547e-05     1.02101         0.53464      \n","500             1.01274         0.53333         0.00380         4.77956e-05     1.02025         0.53391      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0170039 0.0047450867\n","\n","Final train acc (mean/std across restarts so far):\n","0.53158045 0.0023711803\n","\n","Final test mse (mean/std across restarts so far):\n","1.0289332 0.005432877\n","\n","Final test acc (mean/std across restarts so far):\n","0.5282627 0.0031476677\n","\n","-------------------------------------- Restart 7 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.56113         0.48825         1.22233         0.00103         1.60092         0.49596      \n","100             1.03983         0.53957         0.01938         0.00096         1.05393         0.52503      \n","200             1.01972         0.53222         0.00614         6.61551e-05     1.03677         0.52526      \n","300             1.02072         0.53127         0.00614         5.28368e-05     1.03627         0.52321      \n","400             1.02050         0.53049         0.00601         5.82611e-05     1.03607         0.52542      \n","500             1.01946         0.53060         0.00575         6.15696e-05     1.03483         0.52609      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0173544 0.004476248\n","\n","Final train acc (mean/std across restarts so far):\n","0.5314403 0.0022219645\n","\n","Final test mse (mean/std across restarts so far):\n","1.0297748 0.005436036\n","\n","Final test acc (mean/std across restarts so far):\n","0.52795196 0.0030119163\n","\n","-------------------------------------- Restart 8 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.47206         0.52411         1.00146         0.01332         1.56620         0.48360      \n","100             1.02305         0.54418         0.01167         0.00099         1.03652         0.53050      \n","200             1.00918         0.53446         0.00321         3.98242e-05     1.02347         0.52951      \n","300             1.00863         0.53550         0.00310         3.75768e-05     1.02316         0.52861      \n","400             1.01010         0.53447         0.00324         4.91146e-05     1.02261         0.53125      \n","500             1.00925         0.53351         0.00304         5.12907e-05     1.02247         0.53101      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0163418 0.004971004\n","\n","Final train acc (mean/std across restarts so far):\n","0.5316994 0.0021886106\n","\n","Final test mse (mean/std across restarts so far):\n","1.0288615 0.0056299167\n","\n","Final test acc (mean/std across restarts so far):\n","0.5283339 0.0029931786\n","\n","-------------------------------------- Restart 9 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.65826         0.49324         1.78540         0.00332         1.83540         0.45149      \n","100             1.03900         0.54068         0.01925         0.00092         1.05243         0.52671      \n","200             1.02160         0.53569         0.00712         5.83872e-05     1.03949         0.52400      \n","300             1.02214         0.53259         0.00702         5.30222e-05     1.03940         0.52509      \n","400             1.02057         0.53283         0.00661         5.83143e-05     1.03901         0.52328      \n","500             1.02204         0.53169         0.00668         4.66694e-05     1.03887         0.52182      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0169749 0.00501722\n","\n","Final train acc (mean/std across restarts so far):\n","0.5316987 0.0020634427\n","\n","Final test mse (mean/std across restarts so far):\n","1.0299736 0.0061700274\n","\n","Final test acc (mean/std across restarts so far):\n","0.5276104 0.0034858647\n","\n","-------------------------------------- Restart 10 --------------------------------------\n","\n","training on 3 environments (using 37 features):\n","    2014 with 3228 samples\n","    2015 with 2458 samples\n","    2016 with 3158 samples\n","\n","testing on:\n","    2017 with 2736 samples\n","    2018 with 2692 samples\n","\n","\n","step          train mse       train acc       irm penalty     rex penalty     test mse        test acc     \n","0               1.70927         0.47008         1.86428         0.00666         1.69124         0.52140      \n","100             1.03528         0.54088         0.01714         0.00100         1.04601         0.52880      \n","200             1.01793         0.53062         0.00496         5.29559e-05     1.02563         0.53494      \n","300             1.01872         0.53161         0.00494         5.06364e-05     1.02475         0.53616      \n","400             1.01947         0.52972         0.00502         4.75747e-05     1.02515         0.53568      \n","500             1.01880         0.53143         0.00491         5.15697e-05     1.02424         0.53497      \n","\n","\n","Final train mse (mean/std across restarts so far):\n","1.0171573 0.004791058\n","\n","Final train acc (mean/std across restarts so far):\n","0.53167194 0.0019591958\n","\n","Final test mse (mean/std across restarts so far):\n","1.0294002 0.0061009796\n","\n","Final test acc (mean/std across restarts so far):\n","0.52834624 0.0039760903\n"]}]},{"cell_type":"markdown","source":["# For anchor regression:"],"metadata":{"id":"SeQVhUe_tFdT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXSi5rzuY6GF"},"outputs":[],"source":["# Yahuan Zheng: Define the function to perform anchor transformation\n","\n","def anchor_transform(x, y, x_anchor, gamma):\n","    '''\n","      x: a dataframe of input data\n","      y: a dataframe of labels\n","      x_anchor: a dataframe representing the data of the anchors\n","      gamma: a scalar regularization parameter\n","    '''\n","    device = set_device()\n","    n_samples = len(x)\n","    dim = len(x[0])\n","    id = torch.eye(n_samples).to(device)\n","\n","    anchor_data = torch.Tensor(x_anchor).to(device)\n","    a_t_a = torch.t(anchor_data) @ anchor_data\n","    a_t_a_inv = torch.linalg.inv(a_t_a)\n","    proj_mat = (anchor_data @ a_t_a_inv) @ torch.t(anchor_data)\n","\n","    left_x = torch.sub(id, proj_mat) @ x\n","    left_y = torch.sub(id, proj_mat) @ y\n","    right_x = torch.mul(proj_mat @ x, np.sqrt(gamma))\n","    right_y = torch.mul(proj_mat @ y, np.sqrt(gamma))\n","\n","    x_trans = torch.add(left_x, right_x)\n","    y_trans = torch.add(left_y, right_y)\n","\n","    return x_trans, y_trans"]},{"cell_type":"code","source":["# Yahuan Zheng: Anchor regression 1st try (brute force way)\n","# Use CLASS LABEL as the target variable\n","\n","train_envs = get_years(train_env_ids, anchors=['Sector_label'])     # Yahuan Zheng: choose the anchors here\n","test_envs = get_years(test_env_ids)\n","regs = np.linspace(0, 20, num=201, endpoint=True)\n","best_reg = 1e6\n","best_acc = 0.0\n","\n","# preprocess\n","for e in train_envs + test_envs:\n","  e['features'] = whiten(e['features'])\n","  e['anchors'] = whiten(e['anchors'])\n","\n","pretty_print('gamma', 'train_acc', 'test_acc')\n","\n","for gamma in regs:\n","    x_train = []\n","    y_train = []\n","    x_train_trans = []\n","    y_train_trans = []\n","    x_test = []\n","    y_test = []\n","\n","    for e in train_envs:\n","      x, y = e['features'], e['labels']     # Using CLASS LABEL as target variable\n","      x_trans, y_trans = anchor_transform(e['features'], e['labels'], e['anchors'], gamma=gamma)\n","      x_train.append(x.cpu().numpy())\n","      y_train.append(y.cpu().numpy())\n","      x_train_trans.append(x_trans.cpu().numpy())\n","      y_train_trans.append(y_trans.cpu().numpy())\n","\n","    for e in test_envs:\n","      x_test.append(e['features'].cpu().numpy())\n","      y_test.append(e['labels'].cpu().numpy())\n","\n","    x_train = np.vstack(x_train)\n","    y_train = np.vstack(y_train)\n","    x_train_trans = np.vstack(x_train_trans)\n","    y_train_trans = np.vstack(y_train_trans)\n","    x_test = np.vstack(x_test)\n","    y_test = np.vstack(y_test)\n","\n","    anchor_reg = LinearRegression(fit_intercept=False).fit(x_train_trans, y_train_trans)\n","    w = anchor_reg.coef_\n","    y_hat = anchor_reg.predict(x_test)\n","    y_hat_label = (y_hat > 0.5).astype(float)      # Predict the labels based on a threshold value of 0.5\n","    #train_mse = mean_squared_error(y_train, anchor_reg.predict(x_train))\n","    #test_mse = mean_squared_error(y_test, anchor_reg.predict(x_test))\n","    train_acc = (abs((y_train > 0.).astype(float) - (anchor_reg.predict(x_train) > 0.5).astype(float)) < 1e-2).astype(float).mean()\n","    test_acc = (abs(y_hat_label - (y_test > 0.).astype(float))< 1e-2).astype(float).mean()\n","\n","    pretty_print(gamma, train_acc, test_acc)\n","    #print('--- Anchor regression with gamma = {:.1f} has accuracy {:.5f}.'.format(gamma, anchor_acc_test))\n","\n","    '''\n","    if gamma % 10 == 0:\n","        plt.hist(y_hat, bins='auto', density=True)\n","        plt.title('Histogram of y_hat for Anchor regression with gamma={:.1f}'.format(gamma))\n","        plt.show()\n","    '''\n","\n","    if test_acc > best_acc:\n","        best_reg = gamma\n","        best_acc = test_acc\n","\n","print('\\nBest reg is gamma = {:.1f} with {:.5f} accuracy.'.format(best_reg, best_acc))\n","\n"],"metadata":{"id":"BEYIzDZiU0fE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1q5NyoVY6gu"},"outputs":[],"source":["# Yahuan Zheng: Anchor regression 2nd try\n","# Use ANNUAL RETURN as the target variable\n","\n","train_envs = get_years(train_env_ids, anchors=['Sector_label'])     # Yahuan Zheng: choose the anchors here\n","test_envs = get_years(test_env_ids)\n","regs = np.linspace(0, 20, num=201, endpoint=True)\n","best_reg = 1e6\n","best_acc = 0.0\n","\n","# preprocess\n","for e in train_envs + test_envs:\n","  e['features'] = whiten(e['features'])\n","  e['anchors'] = whiten(e['anchors'])\n","  e['returns'] = whiten(e['returns'])\n","\n","pretty_print('gamma', 'train mse', 'train_acc', 'test mse', 'test_acc')\n","\n","for gamma in regs:\n","    x_train = []\n","    y_train = []\n","    x_train_trans = []\n","    y_train_trans = []\n","    x_test = []\n","    y_test = []\n","\n","    for e in train_envs:\n","      x, y = e['features'], e['returns']\n","      x_trans, y_trans = anchor_transform(e['features'], e['returns'], e['anchors'], gamma=gamma)\n","      x_train.append(x.cpu().numpy())\n","      y_train.append(y.cpu().numpy())\n","      x_train_trans.append(x_trans.cpu().numpy())\n","      y_train_trans.append(y_trans.cpu().numpy())\n","\n","    for e in test_envs:\n","      x_test.append(e['features'].cpu().numpy())\n","      y_test.append(e['returns'].cpu().numpy())\n","\n","    x_train = np.vstack(x_train)\n","    y_train = np.vstack(y_train)\n","    x_train_trans = np.vstack(x_train_trans)\n","    y_train_trans = np.vstack(y_train_trans)\n","    x_test = np.vstack(x_test)\n","    y_test = np.vstack(y_test)\n","\n","    anchor_reg = LinearRegression(fit_intercept=False).fit(x_train_trans, y_train_trans)\n","    w = anchor_reg.coef_\n","    y_hat = anchor_reg.predict(x_test)\n","    y_hat_label = (y_hat > 0.).astype(float)\n","    train_mse = mean_squared_error(y_train, anchor_reg.predict(x_train))\n","    test_mse = mean_squared_error(y_test, anchor_reg.predict(x_test))\n","    train_acc = (abs((y_train > 0.).astype(float) - (anchor_reg.predict(x_train) > 0.).astype(float)) < 1e-2).astype(float).mean()\n","    test_acc = (abs(y_hat_label - (y_test > 0.).astype(float))< 1e-2).astype(float).mean()\n","\n","    pretty_print(gamma, train_mse, train_acc, test_mse, test_acc)\n","    #print('--- Anchor regression with gamma = {:.1f} has accuracy {:.5f}.'.format(gamma, anchor_acc_test))\n","\n","    '''\n","    if gamma % 10 == 0:\n","        plt.hist(y_hat, bins='auto', density=True)\n","        plt.title('Histogram of y_hat for Anchor regression with gamma={:.1f}'.format(gamma))\n","        plt.show()\n","    '''\n","\n","    if test_acc > best_acc:\n","        best_reg = gamma\n","        best_acc = test_acc\n","\n","print('\\nBest reg is gamma = {:.1f} with {:.5f} accuracy.'.format(best_reg, best_acc))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHyjxrArMDlG"},"outputs":[],"source":["### Optional (applicable for command line executions)\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import numpy.lib as npl\n","import sys\n","#from load import YEARS\n","\n","EXP_LIST = [[2014, 2015, 2016]]\n","\"\"\"\n","EXP_LIST = [\n","    [2014, 2015, 2016],\n","    [2014, 2015, 2017],\n","    [2014, 2015, 2018],\n","    [2014, 2016, 2017],\n","    [2014, 2016, 2018],\n","    [2014, 2017, 2018],\n","    [2015, 2016, 2017],\n","    [2015, 2016, 2018],\n","    [2015, 2017, 2018],\n","    [2016, 2017, 2018],\n","]\n","\"\"\"\n","\n","def plot_data(means, stds, descr, plot_every=8, title='', ylabel=''):\n","    fig, ax = plt.subplots(1, figsize=(4,3))\n","    for m, s, info in zip(means, stds, descr):\n","        num_el = len(m)\n","        x = np.arange(num_el)\n","        x = x[:-(num_el%plot_every)].reshape(-1,plot_every).mean(1)\n","        y = m[:-(num_el%plot_every)].reshape(-1,plot_every).mean(1)\n","        dy = s[:-(num_el%plot_every)].reshape(-1,plot_every).mean(1)\n","        ax.fill_between(x, y-dy, y+dy, alpha=0.25)\n","        ax.plot(x, y, label=info)\n","        ax.set_title(title)\n","        ax.set_ylabel(ylabel)\n","        ax.set_xlabel(\"Iteration\")\n","\n","    fig.tight_layout()\n","    plt.legend()\n","\n","\n","if __name__ == \"__main__\":\n","\n","    if sys.argv[-1] == 'all':\n","        # combine\n","        mat_erm, mat_irm, mat_rex = [np.loadtxt('results/%s_matrix.dat' % k) for k in ['erm', 'irm', 'rex']]\n","        mat_train = mat_erm[:, :len(YEARS)] / mat_erm[:, :len(YEARS)]\n","        mat_erm = mat_erm[:, len(YEARS):]\n","        mat_irm = mat_irm[:, len(YEARS):]\n","        mat_rex = mat_rex[:, len(YEARS):]\n","\n","        irm_rel = 100 * (mat_irm - mat_erm) / mat_erm\n","        rex_rel = 100 * (mat_rex - mat_erm) / mat_erm\n","\n","        notnan = irm_rel > -999\n","        vmax = np.abs([irm_rel[notnan].min(), irm_rel[notnan].max(), rex_rel[notnan].min(), rex_rel[notnan].max()]).max()\n","\n","        e = mat_erm[notnan]\n","        i = mat_irm[notnan]\n","        r = mat_rex[notnan]\n","        ir = irm_rel[notnan]\n","        rr = rex_rel[notnan]\n","\n","        print('cases irm better', (ir > 0).mean())\n","        print('cases rex better', (rr > 0).mean())\n","        print('irm rel performance (mean, std, min, max)', ir.mean(), ir.std(), ir.min(), ir.max())\n","        print('rex rel performance (mean, std, min, max)', rr.mean(), rr.std(), rr.min(), rr.max())\n","        print('erm performance (mean, std, min, max)', e.mean(), e.std(), e.min(), e.max())\n","        print('irm performance (mean, std, min, max)', i.mean(), i.std(), i.min(), i.max())\n","        print('rex performance (mean, std, min, max)', r.mean(), r.std(), r.min(), r.max())\n","\n","        x, y = np.meshgrid(range(5), range(10))\n","        x = x[mat_train == 1]\n","        y = y[mat_train == 1]\n","\n","        ax = plt.subplot(131)\n","        plt.grid(zorder=-99)\n","        plt.imshow(mat_train, cmap='binary')\n","        plt.scatter(x, y, c='k', zorder=10)\n","        plt.xticks(range(len(YEARS)), YEARS)\n","        plt.yticks(range(10), ['']*10)\n","        plt.title('Training envs.')\n","        plt.ylabel('Task')\n","        plt.xticks(rotation=45)\n","        plt.xlim(-0.5, 4.5)\n","        plt.ylim(9.5, -0.5)\n","\n","        ax = plt.subplot(132)\n","        plt.imshow(irm_rel, cmap='PiYG', vmin=-vmax, vmax=vmax, zorder=1)\n","        plt.scatter(x, y, c='k', zorder=10)\n","        plt.xticks(range(len(YEARS)), YEARS)\n","        plt.yticks(range(10), ['']*10)\n","        plt.title('Test IRM')\n","        plt.xticks(rotation=45)\n","\n","        ax = plt.subplot(133)\n","        im = plt.imshow(rex_rel, cmap='PiYG', vmin=-vmax, vmax=vmax, zorder=1)\n","        plt.scatter(x, y, c='k', zorder=10)\n","        plt.xticks(range(len(YEARS)), YEARS)\n","        plt.yticks(range(10), ['']*10)\n","        plt.title('Test REx')\n","        plt.xticks(rotation=45)\n","\n","\n","        cbar = plt.colorbar(im)\n","        cbar.ax.set_ylabel('Performance relative to ERM (%)', rotation=90)\n","\n","        plt.savefig('grid.pdf')\n","        plt.show()\n","        exit()\n","\n","    # plot single experiment\n","    train_envs = [np.array(e, dtype=int) for e in EXP_LIST]\n","    test_envs = [npl.setxor1d(YEARS, e) for e in train_envs]\n","\n","    fnames = ['results/%s_%s_%s.npz' % ('',\n","        ','.join([str(e) for e in train_env]),\n","        ','.join([str(e) for e in test_env])) \\\n","            for train_env, test_env in zip(train_envs, test_envs)]\n","    print(fnames)\n","\n","    files = [np.load(fname) for fname in fnames]\n","    SMOOTH = 50\n","\n","    train_means = [f['train_acc'][:,-SMOOTH:].mean() for f in files]\n","    train_stds = [f['train_acc'][:,-SMOOTH:].std() for f in files]\n","\n","    max_pts = [f['test_acc'].mean(0)[:-1].reshape((-1, SMOOTH, 2)).mean(1).argmax(0).clip(1, np.inf) * SMOOTH for f in files]\n","    max_pts = [p.astype(int)[::-1] for p in max_pts]\n","\n","    print('stopping points:', max_pts)\n","\n","    test_means = [\n","        np.stack([\n","        f['test_acc'][:, pts[i]-SMOOTH//2:pts[i]+SMOOTH//2, i].mean() \\\n","            for i in [0,1]]) \\\n","                for f,pts in zip(files, max_pts)]\n","\n","    out_img = np.zeros((len(train_envs), 2*len(YEARS)))\n","\n","    for i, (e_tr, m_tr, e_te, m_te) in enumerate(zip(train_envs, train_means, test_envs, test_means)):\n","        out_img[i, e_tr-YEARS[0]] = m_tr\n","        out_img[i, len(YEARS)-YEARS[0]+e_te] = m_te\n","\n","    np.savetxt('results/matrix.dat', out_img)\n","    #np.savetxt('results/%s_matrix.dat' % sys.argv[-1], out_img)\n","    plt.imshow(out_img)\n","    plt.show()\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1bA3JBLDg9b958LhniH0xW-2Nu3MNYb10","authorship_tag":"ABX9TyNQfycUmbRZ1dlFO13V0KNi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}